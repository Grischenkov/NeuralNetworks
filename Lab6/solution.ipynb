{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "NbIUG9ZTOsVi"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import torch\n",
        "import urllib\n",
        "import zipfile\n",
        "import pandas as pd\n",
        "\n",
        "from PIL import Image\n",
        "from tqdm import tqdm\n",
        "from datasets import load_metric\n",
        "from torch.utils.data import Dataset\n",
        "from transformers import TrOCRProcessor\n",
        "from transformers import Seq2SeqTrainer\n",
        "from transformers import default_data_collator\n",
        "from transformers import Seq2SeqTrainingArguments\n",
        "from transformers import VisionEncoderDecoderModel\n",
        "from sklearn.model_selection import train_test_split"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Ð—Ð°Ð³Ñ€ÑƒÐ·ÐºÐ° Ð´Ð°Ð½Ð½Ñ‹Ñ…"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "kh3MjBqnOsVj"
      },
      "outputs": [],
      "source": [
        "!mkdir data\n",
        "urllib.request.urlretrieve('https://storage.yandexcloud.net/datasouls-ods/materials/46b7bb85/datasets.zip', 'data/datasets.zip')\n",
        "with zipfile.ZipFile(\"data/datasets.zip\", \"r\") as f:\n",
        "    f.extractall(\"data/\")"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# ÐŸÐ¾Ð´Ð³Ð¾Ñ‚Ð¾Ð²ÐºÐ° Ð´Ð°Ð½Ð½Ñ‹Ñ…"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BPWN5X9qOsVk",
        "outputId": "4ed70bc9-3d18-4242-a866-52a38b2ecec7"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 6196/6196 [00:00<00:00, 46237.22it/s]\n"
          ]
        }
      ],
      "source": [
        "filespath = 'data/train/'\n",
        "filenames_ims = sorted(next(os.walk(filespath+'images'), (None, None, []))[2])\n",
        "filenames_words = sorted(next(os.walk(filespath+'words'), (None, None, []))[2])\n",
        "texts = []\n",
        "for filename in tqdm((filenames_words)):\n",
        "    with open(filespath +'/words/' + filename) as f:\n",
        "        texts.append(f.readline())\n",
        "df =  pd.DataFrame({'file_name': filenames_ims, 'text': texts})"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "3PFAL92mOsVk"
      },
      "outputs": [],
      "source": [
        "train_df, test_df = train_test_split(df[:1000], test_size=0.1, shuffle=True)\n",
        "train_df.reset_index(drop=True, inplace=True)\n",
        "test_df.reset_index(drop=True, inplace=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "g6aafKNUOsVl"
      },
      "outputs": [],
      "source": [
        "class IAMDataset(Dataset):\n",
        "    def __init__(self, root_dir, df, processor, max_target_length=128):\n",
        "        self.root_dir = root_dir\n",
        "        self.df = df\n",
        "        self.processor = processor\n",
        "        self.max_target_length = max_target_length\n",
        "    def __len__(self):\n",
        "        return len(self.df)\n",
        "    def __getitem__(self, idx):\n",
        "        file_name = self.df['file_name'][idx]\n",
        "        text = self.df['text'][idx]\n",
        "        image = Image.open(self.root_dir + file_name).convert(\"RGB\")\n",
        "        pixel_values = self.processor(image, return_tensors=\"pt\").pixel_values\n",
        "        labels = self.processor.tokenizer(text, padding=\"max_length\", max_length=self.max_target_length).input_ids\n",
        "        labels = [label if label != self.processor.tokenizer.pad_token_id else -100 for label in labels]\n",
        "        encoding = {\"pixel_values\": pixel_values.squeeze(), \"labels\": torch.tensor(labels)}\n",
        "        return encoding"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MCZleQ8zOsVl",
        "outputId": "da32e5d8-7a8f-4577-989a-802b1daa53d8"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Could not find image processor class in the image processor config or the model config. Loading based on pattern matching with the model's feature extractor configuration.\n"
          ]
        }
      ],
      "source": [
        "processor = TrOCRProcessor.from_pretrained('microsoft/trocr-base-stage1')\n",
        "train_dataset = IAMDataset(root_dir='data/train/images/', df=train_df, processor=processor)\n",
        "eval_dataset = IAMDataset(root_dir='data/train/images/', df=test_df, processor=processor)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "27N9Xo6MOsVl",
        "outputId": "20dea320-0799-424d-e4fa-02eb931e53c8"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "pixel_values torch.Size([3, 384, 384])\n",
            "labels torch.Size([128])\n",
            "ÐºÐ¾Ð»ÐºÐ¾ Ð»Ð¸Ð¼Ð¾Ð½Ð¾Ñ„ ÑÐ²Ñ£Ð¶Ð¸Ñ… Ð° Ñ‡Ñ‚Ð¾ ÑˆÑƒ\n"
          ]
        }
      ],
      "source": [
        "encoding = train_dataset[0]\n",
        "for k,v in encoding.items():\n",
        "    print(k, v.shape)\n",
        "labels = encoding['labels']\n",
        "labels[labels == -100] = processor.tokenizer.pad_token_id\n",
        "label_str = processor.decode(labels, skip_special_tokens=True)\n",
        "print(label_str)"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Ð—Ð°Ð³Ñ€ÑƒÐ·ÐºÐ° Ð¸ Ð¾Ð±ÑƒÑ‡ÐµÐ½Ð¸Ðµ Ð¼Ð¾Ð´ÐµÐ»Ð¸"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hHr9eExNOsVm",
        "outputId": "9459f382-590a-4881-cb79-d86da3e0749a"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Some weights of VisionEncoderDecoderModel were not initialized from the model checkpoint at microsoft/trocr-base-stage1 and are newly initialized: ['encoder.pooler.dense.weight', 'encoder.pooler.dense.bias']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        }
      ],
      "source": [
        "model = VisionEncoderDecoderModel.from_pretrained(\"microsoft/trocr-base-stage1\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "MdnCT1xQOsVm"
      },
      "outputs": [],
      "source": [
        "model.config.decoder_start_token_id = processor.tokenizer.cls_token_id\n",
        "model.config.pad_token_id = processor.tokenizer.pad_token_id\n",
        "model.config.vocab_size = model.config.decoder.vocab_size\n",
        "model.config.eos_token_id = processor.tokenizer.sep_token_id\n",
        "model.config.max_length = 64\n",
        "model.config.early_stopping = True\n",
        "model.config.no_repeat_ngram_size = 3\n",
        "model.config.length_penalty = 2.0\n",
        "model.config.num_beams = 4"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "FWcVVZhmOsVm"
      },
      "outputs": [],
      "source": [
        "training_args = Seq2SeqTrainingArguments(\n",
        "    predict_with_generate=True,\n",
        "    evaluation_strategy=\"steps\",\n",
        "    per_device_train_batch_size=1,\n",
        "    per_device_eval_batch_size=1,\n",
        "    fp16=True, \n",
        "    output_dir=\"./models_dir\",\n",
        "    num_train_epochs=3,\n",
        "    save_steps=10350000,\n",
        "    eval_steps=1035,\n",
        "    metric_for_best_model=\"cer\",\n",
        "    greater_is_better=False\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_eou7rjUOsVn",
        "outputId": "0a765626-77cf-4c0e-b70b-15013ce5225d"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "<ipython-input-12-8e9758749de9>:1: FutureWarning: load_metric is deprecated and will be removed in the next major version of datasets. Use 'evaluate.load' instead, from the new library ðŸ¤— Evaluate: https://huggingface.co/docs/evaluate\n",
            "  cer_metric = load_metric(\"cer\")\n"
          ]
        }
      ],
      "source": [
        "cer_metric = load_metric(\"cer\")\n",
        "wer_metric = load_metric(\"wer\")\n",
        "def compute_string_acc(predictions, references):\n",
        "  string_acc = 0\n",
        "  for preds, refs in zip(predictions, references):\n",
        "    if preds==refs:\n",
        "      string_acc = string_acc + 1\n",
        "  return string_acc/len(predictions)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "BCdQC_yFOsVn"
      },
      "outputs": [],
      "source": [
        "def compute_metrics(pred):\n",
        "    labels_ids = pred.label_ids\n",
        "    pred_ids = pred.predictions\n",
        "\n",
        "    pred_str = processor.batch_decode(pred_ids, skip_special_tokens=True)\n",
        "    labels_ids[labels_ids == -100] = processor.tokenizer.pad_token_id\n",
        "    label_str = processor.batch_decode(labels_ids, skip_special_tokens=True)\n",
        "    # print(pred_str, label_str)\n",
        "    cer = cer_metric.compute(predictions=pred_str, references=label_str)\n",
        "    wer = wer_metric.compute(predictions=pred_str, references=label_str)\n",
        "    string_accuracy = compute_string_acc(predictions=pred_str, references=label_str)\n",
        "\n",
        "    return {\"cer\": cer, \"wer\" : wer, \"string acc\" : string_accuracy}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 575
        },
        "id": "h27PBbTzOsVn",
        "outputId": "9fd72e59-b582-431a-d745-3676a691a49d"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Using cuda_amp half precision backend\n",
            "/usr/local/lib/python3.8/dist-packages/transformers/optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
            "  warnings.warn(\n",
            "***** Running training *****\n",
            "  Num examples = 900\n",
            "  Num Epochs = 3\n",
            "  Instantaneous batch size per device = 1\n",
            "  Total train batch size (w. parallel, distributed & accumulation) = 1\n",
            "  Gradient Accumulation steps = 1\n",
            "  Total optimization steps = 2700\n",
            "  Number of trainable parameters = 384864768\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='2700' max='2700' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [2700/2700 21:46, Epoch 3/3]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Step</th>\n",
              "      <th>Training Loss</th>\n",
              "      <th>Validation Loss</th>\n",
              "      <th>Cer</th>\n",
              "      <th>Wer</th>\n",
              "      <th>String acc</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>1035</td>\n",
              "      <td>3.714800</td>\n",
              "      <td>3.662571</td>\n",
              "      <td>0.846101</td>\n",
              "      <td>1.827027</td>\n",
              "      <td>0.010000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2070</td>\n",
              "      <td>3.582900</td>\n",
              "      <td>3.614345</td>\n",
              "      <td>2.460663</td>\n",
              "      <td>4.920721</td>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table><p>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "***** Running Evaluation *****\n",
            "  Num examples = 100\n",
            "  Batch size = 1\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 100\n",
            "  Batch size = 1\n",
            "\n",
            "\n",
            "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
            "\n",
            "\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "TrainOutput(global_step=2700, training_loss=3.687672051323785, metrics={'train_runtime': 1308.9146, 'train_samples_per_second': 2.063, 'train_steps_per_second': 2.063, 'total_flos': 2.389217716784333e+18, 'train_loss': 3.687672051323785, 'epoch': 3.0})"
            ]
          },
          "execution_count": 14,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "trainer = Seq2SeqTrainer(\n",
        "    model=model,\n",
        "    tokenizer=processor.image_processor_class,\n",
        "    args=training_args,\n",
        "    compute_metrics=compute_metrics,\n",
        "    train_dataset=train_dataset,\n",
        "    eval_dataset=eval_dataset,\n",
        "    data_collator=default_data_collator,\n",
        ")\n",
        "trainer.train()"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": []
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "NN_Lab6",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.9 | packaged by conda-forge | (main, Jan 11 2023, 15:15:40) [MSC v.1916 64 bit (AMD64)]"
    },
    "orig_nbformat": 4,
    "vscode": {
      "interpreter": {
        "hash": "da6ac506b2783abd19b4b6309bf9cdc0a62a1e6d2a22c3f23df86887753b31d5"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
